# Fine-Tuning Large Language Models with Low-Rank Adaptation
Overview
This project focuses on fine-tuning large language models such as BERT and RoBERTa for specific downstream tasks by tweaking only a portion of the parameters. Traditional fine-tuning methods, which involve adjusting all the parameters, can be highly time-consuming. Instead, we employ the Low-Rank Adaptation (LoRA) technique to achieve comparable accuracy with significantly reduced computational time.


